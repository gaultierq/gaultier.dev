<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Blog Name</title>
  <subtitle>Blog subtitle</subtitle>
  <id>http://blog.url.com/</id>
  <link href="http://blog.url.com/"/>
  <link href="http://blog.url.com/feed.xml" rel="self"/>
  <updated>2025-04-08T21:46:00+02:00</updated>
  <author>
    <name>Blog Author</name>
  </author>
  <entry>
    <title>How to Shield Your App from a Rogue Sidekiq Job</title>
    <link rel="alternate" href="http://blog.url.com/2025/04/08/sidekiq-long-running-job.html"/>
    <id>http://blog.url.com/2025/04/08/sidekiq-long-running-job.html</id>
    <published>2025-04-08T21:46:00+02:00</published>
    <updated>2025-05-08T16:17:02+02:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;h1 id="how-to-shield-your-app-from-a-rogue-sidekiq-job"&gt;How to Shield Your App from a Rogue Sidekiq Job&lt;/h1&gt;

&lt;p&gt;Sidekiq is great for background processing—but a single misbehaving job (infinite loops, massive data scans, N+1 storms…) can choke your entire pipeline. Restarting Sidekiq only brings the offender right back. &lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How do you quarantine a bad job so that it can’t block all your critical work?&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id="a-real-world-incident"&gt;A Real-World Incident&lt;/h2&gt;

&lt;p&gt;A user submitted a report with a huge date range—no sanitization—and our &lt;code class="prettyprint"&gt;ReportJob&lt;/code&gt; tried to load terabytes of data. CPU spiked, threads stalled, and everything ground to a halt. Killing and restarting Sidekiq pods simply requeued the same job, and the outage persisted. We needed a way to isolate that single problematic worker.&lt;/p&gt;

&lt;h2 id="introducing-a-quarantine-queue"&gt;Introducing a “Quarantine” Queue&lt;/h2&gt;

&lt;p&gt;The idea is simple: route suspect jobs into a low-concurrency queue serviced by a dedicated Sidekiq process. They’re still retried, but can’t block your high-priority queues.&lt;/p&gt;

&lt;h3 id="1-moving-jobs-to-the-quarantine-queue-manually"&gt;1. Moving jobs to the quarantine queue manually&lt;/h3&gt;

&lt;p&gt;We begin by intercepting job enqueuing and swapping their queue name if they match an ENV-driven “quarantine” list:&lt;/p&gt;

&lt;pre&gt;&lt;code class="prettyprint lang-ruby"&gt;class QuarantineMiddleware
  def call(worker, job, queue, redis_pool)
    if ENV.fetch(&amp;#39;QUARANTINED_JOBS&amp;#39;, &amp;#39;&amp;#39;).split(&amp;#39;;&amp;#39;).include?(job[&amp;#39;class&amp;#39;])
      job[&amp;#39;queue&amp;#39;] = &amp;#39;quarantine&amp;#39;
    end
    yield
  end
end

Sidekiq.configure_client do |config|
  config.client_middleware { |chain| chain.add QuarantineMiddleware }
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Set
&lt;code class="prettyprint"&gt;
QUARANTINED_JOBS=ReportJob;MegaLoopJob
&lt;/code&gt;
to start shunting those jobs into quarantine.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=bvdWPGQ8cEA&amp;amp;t=229s"&gt;Inspired by this deep-dive on isolating memory-leak jobs.&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id="2-handling-infinite-loops-on-restart"&gt;2. Handling Infinite Loops on Restart&lt;/h3&gt;

&lt;p&gt;That client middleware only works on new enqueues; it can’t catch jobs Sidekiq auto-requeues on shutdown. To trap long-running jobs after a pod restart, we use a server middleware:&lt;/p&gt;

&lt;pre&gt;&lt;code class="prettyprint lang-ruby"&gt;class LongRunningJobInterceptor
  QUARANTINE_QUEUE = &amp;#39;quarantine&amp;#39;

  def call(worker, job, queue)
    jid = job[&amp;#39;jid&amp;#39;]
    if queue != QUARANTINE_QUEUE &amp;amp;&amp;amp; should_quarantine?(jid)
      # Requeue safely into quarantine
      worker.class.set(queue: QUARANTINE_QUEUE).perform_async(*job[&amp;#39;args&amp;#39;])
      return
    end

    track_start(jid)
    begin
      yield
    rescue Sidekiq::Shutdown
      raise
    ensure
      untrack(jid)
    end
  end

  private

  def should_quarantine?(jid)
    start_time = Sidekiq.redis { |r| r.get(&amp;quot;job:#{jid}:started_at&amp;quot;).to_time }
    (Time.current - start_time) &amp;gt; 1.hour
  end

  def track_start(jid)
    Sidekiq.redis { |r| r.set(&amp;quot;job:#{jid}:started_at&amp;quot;, Time.current) }
  end

  def untrack(jid)
    Sidekiq.redis { |r| r.del(&amp;quot;job:#{jid}:started_at&amp;quot;) }
  end
end

Sidekiq.configure_server do |config|
  config.server_middleware { |chain| chain.add LongRunningJobInterceptor }
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, whenever you restart Sidekiq, any job that had been running “too long” automatically moves into your quarantine queue instead of clogging default or critical queues.&lt;/p&gt;

&lt;h3 id="3-automating-detection-restart"&gt;3. Automating Detection &amp;amp; Restart&lt;/h3&gt;

&lt;p&gt;Maintaining a list of known bad actors is helpful, but it still leaves you manually updating ENV variables and restarting processes when things go awry. What if we could automate both detection and remediation of runaway jobs?
Let’s watch for runaway jobs and trigger a graceful process restart.&lt;/p&gt;

&lt;h4 id="3-a-why-not-simple-timeouts"&gt;3.a Why Not Simple Timeouts?&lt;/h4&gt;

&lt;p&gt;Some teams rely on hard timeouts (&lt;code class="prettyprint"&gt;Sidekiq::JobTimeout&lt;/code&gt;) or even OS-level kills. Unfortunately, these can:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Abort mid-transaction, leaving partial writes or queued messages.&lt;/li&gt;
&lt;li&gt;Leak resources (DB connections, file handles), causing pool exhaustion.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id="3-b-soft-timeout-monitoring"&gt;3.b Soft Timeout Monitoring&lt;/h4&gt;

&lt;p&gt;Instead of killing the job mid-flight, let’s signal our orchestrator to restart the entire Sidekiq process, then quarantine repeat offenders on the next run.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Track job start times in a shared registry.&lt;/li&gt;
&lt;li&gt;Periodically scan for jobs exceeding a configurable threshold (e.g. 2 minutes).&lt;/li&gt;
&lt;li&gt;Flag the Sidekiq process as unhealthy (e.g. by touching a file).&lt;/li&gt;
&lt;li&gt;Let Kubernetes or Systemd detect the unhealthy marker and recycle the pod or service.&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class="prettyprint lang-ruby"&gt;class SidekiqMonitor
  HEALTH_FILE = Rails.root.join(&amp;#39;tmp/sidekiq_unhealthy&amp;#39;).freeze

  def self.start
    Thread.new(name: &amp;#39;sidekiq-monitor&amp;#39;) do
      loop do
        RunningJobs.list.each do |job|
          if Time.current - job[:started_at] &amp;gt; 2.minutes
            FileUtils.touch(HEALTH_FILE)
            break
          end
        end
        sleep 1
      end
    end
  end
end

class RunningJobs
  @jobs = Concurrent::Map.new

  def call(worker, job, queue)
    @jobs[Thread.current.object_id] = { jid: job[&amp;#39;jid&amp;#39;], started_at: Time.current }
    yield
  ensure
    @jobs.delete(Thread.current.object_id)
  end

  def self.list
    @jobs.values
  end
end

# In config/initializers/sidekiq.rb
Sidekiq.configure_server do |config|
  config.server_middleware { |chain| chain.add RunningJobs }
  SidekiqMonitor.start
end
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Kubernetes (or Systemd) watches &lt;code class="prettyprint"&gt;tmp/sidekiq_unhealthy&lt;/code&gt; and gracefully restarts the process.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On restart, our &lt;code class="prettyprint"&gt;LongRunningJobInterceptor&lt;/code&gt; (from step 2) reroutes any job that previously ran too long into the quarantine queue.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;By combining:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Client-side queue rerouting (for future enqueues),&lt;/li&gt;
&lt;li&gt;Server-side interception (for jobs requeued on shutdown), and&lt;/li&gt;
&lt;li&gt;Automated health monitoring with liveness probes,&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;you can ensure that one rogue Sidekiq job never brings down your entire background pipeline.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Self DDOS-ing with long-running requests and Nginx</title>
    <link rel="alternate" href="http://blog.url.com/2025/01/11/how-to-self-ddos-using-nginx-retry-mechanism.html"/>
    <id>http://blog.url.com/2025/01/11/how-to-self-ddos-using-nginx-retry-mechanism.html</id>
    <published>2025-01-11T18:59:00+01:00</published>
    <updated>2025-04-24T19:52:26+02:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;p&gt;Last week, I encountered an issue that caused a huge CPU spike on our backend system leading to congestion. 
While the platform stayed up and running, this incident highlighted how a small configuration detail can significantly impact performance. 
Here&amp;#39;s a breakdown of what happened. &lt;/p&gt;

&lt;h2 id="the-incident"&gt;The Incident&lt;/h2&gt;

&lt;p&gt;Our system operates on a Kubernetes cluster and was functioning normally until alerts flagged 100% CPU usage on the node hosting the database.&lt;/p&gt;

&lt;p&gt;The team quickly identified long-running requests executing heavy SQL calculations. These requests were triggered when customers generated yearly reports for their dashboards.&lt;/p&gt;

&lt;p&gt;Ideally, the system should delegate such intensive calculations to background jobs, allowing the requests to return immediately after enqueuing a job. 
However, the implementation was building the report synchronously (within the request lifecycle). That was a first red flag. &lt;/p&gt;

&lt;p&gt;Since generating such a report can take up to 40 minutes, users often assume the system has stalled, and after a few seconds, would hit the refresh button, triggering new requests. &lt;/p&gt;

&lt;p&gt;Over time, these repeated requests overwhelmed the system, exhausting its resources.&lt;/p&gt;

&lt;h2 id="the-investigation"&gt;The investigation&lt;/h2&gt;

&lt;p&gt;To understand the issue, I tested it myself by generating a new yearly report. After about 3 minutes, my browser encountered a 504 proxy timeout error.
Curious about where this timeout was configured, I dug into our nginx ingress controller settings and found: &lt;/p&gt;

&lt;pre&gt;&lt;code class="prettyprint"&gt;proxy_read_timeout 60s;
proxy_next_upstream_tries 3;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hm interesting ! When a user requests a report, nginx forwards the request to our Rails application, 
which then generates lots of database load. If the upgstream server (our web app) doesn’t respond within 60 seconds, nginx retries the request—up to three times. 
At this point, it&amp;#39;s important to note, that if nginx has given up on reading a response, our web server is still processing the request, and will continue for as long as the report generation requires.&lt;/p&gt;

&lt;p&gt;On top of nginx resending requests every 60 seconds while (understandably) frustrated users were refreshing the page, sending additional requests. 
This compounded the problem, overwhelming the system from both internal retries and external user actions.&lt;/p&gt;

&lt;p&gt;While the 40-minute request time is undeniably a design flaw that requires immediate attention, the &lt;strong&gt;ingress controller&amp;#39;s retry logic unintentionally magnified the issue by tripling the load&lt;/strong&gt;. &lt;/p&gt;

&lt;p&gt;This made it even harder for our team to manage the situation effectively.&lt;/p&gt;

&lt;h2 id="the-fix"&gt;The Fix&lt;/h2&gt;

&lt;p&gt;The team quickly agreed that report generation should be handled asynchronously using a background job. 
However, implementing this solution would take some work, and we needed an immediate fix to prevent another CPU spike. 
To mitigate the issue temporarily, we decided to deactivate the nginx retry mechanism. 
While this wouldn’t fully resolve the problem, it would reduce the load by two-thirds.&lt;/p&gt;

&lt;p&gt;Rather than diving into the documentation myself, I opted to consult ChatGPT for guidance. 
The AI confidently suggested setting &lt;code class="prettyprint"&gt;proxy_next_upstream_tries 0;&lt;/code&gt; to disable the retry mechanism entirely. 
Sounded simple enough, right?&lt;/p&gt;

&lt;p&gt;I updated the ingress annotations accordingly and waited for the Nginx pods to reload their configuration. 
Then I ran a new test, expecting to see the request fail with a 504 error after 60 seconds. &lt;/p&gt;

&lt;p&gt;But to my surprise, the request didn’t time out. It continued running for 1, 2, 3, 4 minutes... 
Meanwhile, the database load began to rise again.&lt;/p&gt;

&lt;p&gt;Realizing something was wrong, I quickly rolled back the Helm chart and deleted all the web pods, which restored the CPU usage to normal. 
Disaster (barely) averted.&lt;/p&gt;

&lt;p&gt;Time to read nginx documentation. Here&amp;#39;s what I found regarding &lt;code class="prettyprint"&gt;proxy_next_upstream_tries&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class="prettyprint"&gt;Limits the time during which a request can be passed to the next server. The 0 value turns off this limitation.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By setting &lt;code class="prettyprint"&gt;proxy_next_upstream_tries&lt;/code&gt; to &lt;code class="prettyprint"&gt;0&lt;/code&gt;, I had configured nginx to retry requests indefinitely! 
Every 60 seconds, nginx would resend the request to our backend, compounding the problem exponentially: a single user’s request could have overwhelmed our resources entirely.&lt;/p&gt;

&lt;p&gt;I rushed to update the configuration and set &lt;code class="prettyprint"&gt;proxy_next_upstream_tries&lt;/code&gt; to &lt;code class="prettyprint"&gt;1&lt;/code&gt;, ensuring that Nginx would only try the request once (no retry). &lt;/p&gt;

&lt;p&gt;After 60 seconds, the request did fail gracefully with a 504 error, preventing the backend from being overwhelmed.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Using Partial Indexes in Rails</title>
    <link rel="alternate" href="http://blog.url.com/2024/12/31/partial-index.html"/>
    <id>http://blog.url.com/2024/12/31/partial-index.html</id>
    <published>2024-12-31T01:00:00+01:00</published>
    <updated>2025-04-21T15:47:58+02:00</updated>
    <author>
      <name>Article Author</name>
    </author>
    <content type="html">&lt;h3 id="introduction"&gt;Introduction&lt;/h3&gt;

&lt;p&gt;In this post, I will share the insights I gained and the steps I followed to optimize the performance of a PostgreSQL query on a large table by creating a partial index.&lt;/p&gt;

&lt;p&gt;I was working on improving the speed of a query while keeping my database lightweight and efficient, and I found partial indexing to be an effective solution.&lt;/p&gt;

&lt;h3 id="use-case"&gt;Use case&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Let’s assume we are running a messaging app where &lt;code class="prettyprint"&gt;users&lt;/code&gt; can send messages to each others. On some occasions, our AI detection will flag some flags are &lt;code class="prettyprint"&gt;abusive&lt;/code&gt; and our support team need to review these messages.
Support team complains the list of abusive message they need to review takes a lot of time to load.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;My approach to resolve this issue is the same it would be like fixing any performance bug:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Confirm there is a issue - measure&lt;/li&gt;
&lt;li&gt;re-produce the problem in a consistent way on my local machine&lt;/li&gt;
&lt;li&gt;analyse, experiment, and implement&lt;/li&gt;
&lt;li&gt;deploy&lt;/li&gt;
&lt;li&gt;Check the benefit - measure.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The more challenging difficulty lies in step 2: being able to work locally.&lt;br/&gt;
Fixing a performance issue without measuring is a mistake: often, your intuition will be wrong, and you will optimise the wrong thing.&lt;/p&gt;

&lt;p&gt;So lets go !&lt;/p&gt;

&lt;h2 id="confirm"&gt;Confirm&lt;/h2&gt;

&lt;p&gt;Confirming there was a problem was pretty easy. By navigating to the page using the query, I could easily verify that a loader is displayed for 20 seconds.  &lt;/p&gt;

&lt;p&gt;And a quick look at our logs and I could already see that most of the time was spend in sql.&lt;/p&gt;

&lt;p&gt;At this point, I decided to add an extra log to production, to narrow down to the exact problematic query. This way, we can verify later that whatever optimization we came up with actually had some effect.&lt;/p&gt;

&lt;h2 id="finding-a-way-to-make-the-bug-appear-in-local"&gt;Finding a way to make the bug appear in local&lt;/h2&gt;

&lt;p&gt;The simplest and most effective approach would have been to copy the production data to my disk and work from that. However, the &lt;code class="prettyprint"&gt;messages&lt;/code&gt; table is approximately 1TB (and without partitioning), with several indexes exceeding 100GB each. 
Using the production data wasn&amp;#39;t feasible, I had to create some sample data.&lt;/p&gt;

&lt;h3 id="setting-up-my-local-database"&gt;Setting up my local database&lt;/h3&gt;

&lt;p&gt;In this section I will &lt;span data-expand="docker_postgres_17" class="expander"&gt;setup a local postgres&lt;/span&gt; and generate some random data using a sql query. I need to generate enough data to have the issue arise.&lt;/p&gt;

&lt;div class='hidden' id='docker_postgres_17'&gt;&lt;pre&gt;&lt;code class="prettyprint lang-bash"&gt;docker run --platform linux/arm64 \
  --name postgres17 \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_DB=partial_blog \
  -e POSTGRES_HOST_AUTH_METHOD=trust \
  -p 5417:5432 \
  -d postgres:17

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ol&gt;
&lt;li&gt;&lt;span data-expand="create_messages" class="expander"&gt;create the messages table and some indexes&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;div class='hidden' id='create_messages'&gt;&lt;pre&gt;&lt;code class="prettyprint lang-sql"&gt;DROP TABLE IF EXISTS messages ;

CREATE TABLE messages (
    id SERIAL PRIMARY KEY,           
    created_at TIMESTAMP NOT NULL,   
    updated_at TIMESTAMP NOT NULL,   
    author_id INTEGER NOT NULL,      
    content TEXT NOT NULL,           
    is_abusive BOOLEAN NOT NULL,        -- Whether the message is abusive
    is_spam  BOOLEAN NOT NULL,
    is_reviewed BOOLEAN NOT NULL,       -- Whether the message has been reviewed by support team
    is_archived BOOLEAN NOT NULL
);

CREATE INDEX idx_messages_created_at ON messages (created_at);
CREATE INDEX idx_messages_author_created_at ON messages (author_id, created_at);
CREATE INDEX idx_messages_abusive_reviewed ON messages (is_abusive, is_spam, is_reviewed, is_archived);
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Now that my postgres db is up an running, it’s time to populate my &lt;code class="prettyprint"&gt;messages&lt;/code&gt; table with some data. To do that we will use a sql script, that will generate 10 millions messages.&lt;/p&gt;

&lt;p&gt;I decided to generate messages with the following spread :&lt;/p&gt;

&lt;h3 id="dataset-configuration-1"&gt;&lt;span data-expand="data_set1" class="expander"&gt;dataset configuration #1&lt;/span&gt;&lt;/h3&gt;

&lt;p&gt;&lt;code class="prettyprint"&gt;is_abusive&lt;/code&gt; : 1 in 10,000&lt;/p&gt;

&lt;p&gt;&lt;code class="prettyprint"&gt;is_spam&lt;/code&gt; : 1 in 100,000&lt;/p&gt;

&lt;p&gt;&lt;code class="prettyprint"&gt;is_archived&lt;/code&gt; : 20%&lt;/p&gt;

&lt;p&gt;&lt;code class="prettyprint"&gt;is_reviewed&lt;/code&gt; : 50% of those needing review&lt;/p&gt;

&lt;div class='hidden' id='data_set1'&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;sql&lt;/p&gt;

&lt;pre&gt;&lt;code class="prettyprint lang-sql"&gt;TRUNCATE messages;

WITH existing_max_id AS (
    SELECT
        coalesce(max(id), 0) AS max_id
    FROM
        messages
),
random_data AS (
    SELECT
        (existing_max_id.max_id + row_number() OVER ()) AS id,
        now() - INTERVAL &amp;#39;2 months&amp;#39; + (gs * INTERVAL &amp;#39;40 seconds&amp;#39;) AS created_at,
        (random() &amp;lt; 0.0001) AS is_abusive,
        (random() &amp;lt; 0.00001) AS is_spam,
        (random() &amp;lt; 0.2) AS is_archived,
        &amp;#39;Generated content: &amp;#39; || substring(md5(random()::text) || md5(random()::text)
    FROM 1 FOR (10 + (random() * 50)::int)) AS content -- Random content size
    FROM
        generate_series(1, 10000000) AS gs,
        existing_max_id)
    INSERT INTO messages (id, created_at, updated_at, author_id, content, is_abusive, is_spam, is_reviewed, is_archived)
    SELECT
        id,
        created_at,
        created_at AS updated_at,
        id % 100 + 1 AS author_id, -- Example: author_id based on id (modify as needed)
        content,
        is_abusive,
        is_spam,
        CASE WHEN (is_abusive OR is_spam) THEN
            (random() &amp;lt; 0.5) -- 50% of abusive have been reviewed
        ELSE
            FALSE -- Always false if not abusive
        END AS is_reviewed,
        is_archived
    FROM
        random_data;

&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;On my recent laptop it takes about a minute to populate.&lt;/p&gt;

&lt;p&gt;&lt;span data-expand="create_messages1" class="expander"&gt;Populate with some data: 1% of abusive messages&lt;/span&gt;&lt;/p&gt;

&lt;div class='hidden' id='create_messages1'&gt;&lt;p&gt;My first approach was pretty naive:&lt;/p&gt;

&lt;pre&gt;&lt;code class="prettyprint lang-sql"&gt;WITH existing_max_id AS (
    SELECT COALESCE(MAX(id), 0) AS max_id FROM messages 
),
random_data AS (
    SELECT 
        (existing_max_id.max_id + row_number() OVER ()) AS id,  
        NOW() - INTERVAL &amp;#39;2 months&amp;#39; + (gs * INTERVAL &amp;#39;40 seconds&amp;#39;) AS created_at,
        (random() &amp;lt; 0.01) AS abusive,  -- 1% chance of being abusive
        CASE 
            WHEN (random() &amp;lt; 0.01) THEN (random() &amp;lt; 0.5)  -- 50% of abusive have been reviewed
            ELSE FALSE  -- Always false if not abusive
        END AS reviewed,
        &amp;#39;Generated content: &amp;#39; || substring(md5(random()::text) || md5(random()::text) FROM 1 FOR (10 + (random() * 50)::int)) AS content  -- Random content size
    FROM generate_series(1, 10000000) AS gs,
         existing_max_id
)
INSERT INTO messages (
    id,
    created_at,
    updated_at,
    author_id,
    content,
    abusive,
    reviewed
)
SELECT 
    id,
    created_at,
    created_at AS updated_at,
    id % 100 + 1 AS author_id,  -- Example: author_id based on id (modify as needed)
    content,
    abusive,
    reviewed
FROM random_data;

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;When we to try to retrieve the abusive which hasn’t been reviewed yet, we notice the query is pretty fast: 30ms.
Let’s find out what is going on here, by explaining the query plan.
It appears that postgres only had to examine only 189k rows.&lt;/p&gt;

&lt;p&gt;That’s because my created data has “too many” errors row, postgres will find the 100 rows very quickly, and doesn’t need to examine too much data.&lt;/p&gt;

&lt;h3 id="dataset-configuration-2"&gt;dataset  configuration #2&lt;/h3&gt;

&lt;p&gt;So I decided to make the data a lot more sparse, and went for 1 abusive  message  every 1 million (instead of 1 every 100).&lt;/p&gt;

&lt;p&gt;And now, running the query takes a lot more time: around 250ms. And remember, my test database is around 2GB when my production database is 1TB.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Query plan when there is 1 abusive message every  1 million and 20 millions messages ⇒ 400ms&lt;/li&gt;
&lt;/ul&gt;

&lt;div class='hidden' id='query_plan_1_every_1_million'&gt;&lt;pre&gt;&lt;code class="prettyprint lang-bash"&gt;
    Limit  (cost=371054.44..371054.45 rows=1 width=80) (actual time=368.576..370.384 rows=5 loops=1)
      Output: id, created_at, updated_at, author_id, content, abusive, reviewed
      Buffers: shared hit=192 read=286529
      -&amp;gt;  Sort  (cost=371054.44..371054.45 rows=1 width=80) (actual time=363.242..365.050 rows=5 loops=1)
            Output: id, created_at, updated_at, author_id, content, abusive, reviewed
            Sort Key: messages.created_at DESC
            Sort Method: quicksort  Memory: 26kB
            Buffers: shared hit=192 read=286529
            -&amp;gt;  Gather  (cost=1000.00..371054.43 rows=1 width=80) (actual time=111.675..365.023 rows=8 loops=1)
                  Output: id, created_at, updated_at, author_id, content, abusive, reviewed
                  Workers Planned: 2
                  Workers Launched: 2
                  Buffers: shared hit=192 read=286529
                  -&amp;gt;  Parallel Seq Scan on public.messages  (cost=0.00..370054.33 rows=1 width=80) (actual time=135.916..342.357 rows=3 loops=3)
                        Output: id, created_at, updated_at, author_id, content, abusive, reviewed
                        Filter: (messages.abusive AND (NOT messages.reviewed))
                        Rows Removed by Filter: 6666664
                        Buffers: shared hit=192 read=286529
                        Worker 0:  actual time=3.828..332.174 rows=4 loops=1
                          JIT:
                            Functions: 2
                            Options: Inlining false, Optimization false, Expressions true, Deforming true
                            Timing: Generation 0.143 ms (Deform 0.099 ms), Inlining 0.000 ms, Optimization 0.165 ms, Emission 2.848 ms, Total 3.157 ms
                          Buffers: shared hit=65 read=102685
                        Worker 1:  actual time=292.653..332.188 rows=1 loops=1
                          JIT:
                            Functions: 2
                            Options: Inlining false, Optimization false, Expressions true, Deforming true
                            Timing: Generation 0.134 ms (Deform 0.079 ms), Inlining 0.000 ms, Optimization 0.172 ms, Emission 2.849 ms, Total 3.154 ms
                          Buffers: shared hit=64 read=102368
    Planning Time: 0.177 ms
    JIT:
      Functions: 7
      Options: Inlining false, Optimization false, Expressions true, Deforming true
      Timing: Generation 0.820 ms (Deform 0.448 ms), Inlining 0.000 ms, Optimization 0.701 ms, Emission 10.675 ms, Total 12.196 ms
    Execution Time: 371.023 ms
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We reach our goal: generating a dataset that will let the performance issue arise.&lt;/p&gt;

&lt;h2 id="experiment-analyse"&gt;Experiment - analyse&lt;/h2&gt;

&lt;p&gt;In the previous section, we noticed that data scarcity has a significant impact on performance. But what causes this? What’s going on?&lt;/p&gt;

&lt;p&gt;In both scenarios, the query planner is not using any index, leading it to examine each row sequentially until it identifies 100 rows that meet our filter criteria. Once it finds those 100 rows, it stops and returns the results.&lt;/p&gt;

&lt;p&gt;In the first scenario, where the sample data contains a high number of &lt;code class="prettyprint"&gt;abusive = TRUE&lt;/code&gt; rows, PostgreSQL can still locate the relevant rows relatively quickly, albeit inefficiently.&lt;/p&gt;

&lt;p&gt;On the opposite, in the second scenario, where there is only one matching row for every million, PostgreSQL must scan nearly the entire database to retrieve just five rows, which takes considerably more time.&lt;/p&gt;

&lt;p&gt;Before moving on, I want to copy the database we obtained earlier, especially since we’ll be experimenting with creating indexes and other modifications. This approach allows me to establish a clear reference for my tests: one database reflecting the existing issues and another focused on performance optimisation.&lt;/p&gt;

&lt;h3 id="experiment-1-add-every-booleans-in-an-index"&gt;Experiment 1: add every booleans in an index&lt;/h3&gt;

&lt;p&gt;A first idea would be to create missing indexes. In my experience, many performance issues occurring in small to medium sized team are missing indexes. So the first thing I would look for is a column missing an index.&lt;/p&gt;

&lt;p&gt;In our case, every column appearing in the filter clause has it’s own index. But what if we had an index covering all the columns ? Let’s try that !&lt;/p&gt;

&lt;p&gt;Result: it didn’t help: boolean column cardinality is very low, and index are not optimised for low cardinality. Query planner will not choose this index&lt;/p&gt;

&lt;h3 id="experiment-2-partial-index"&gt;Experiment 2:  partial index&lt;/h3&gt;

&lt;p&gt;A partial index is a specialised index that will only index some rows matching a filter.&lt;/p&gt;

&lt;pre&gt;&lt;code class="prettyprint lang-sql"&gt;CREATE INDEX messages_needing_review_idx
ON messages (id, created_at)
WHERE is_abusive = TRUE or is_spam = TRUE;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This index focuses only on rows which are &lt;code class="prettyprint"&gt;is_abusive = TRUE&lt;/code&gt; or &lt;code class="prettyprint"&gt;is_spam = TRUE&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The filtering is performed at “write time” when new rows are inserted / updated / deleted.&lt;/p&gt;

&lt;p&gt;Now , the query is executed in &lt;code class="prettyprint"&gt;5ms&lt;/code&gt;  which is  30x faster than the initial query.&lt;/p&gt;

&lt;p&gt;It look like we have a serious candidate  for our performance issue.&lt;/p&gt;

&lt;hr/&gt;

&lt;pre&gt;&lt;code class="prettyprint lang-sql"&gt;-&amp;gt; Index Scan using messages_needing_review_idx on messages (cost=0.28..1798.15 rows=802 width=83)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr/&gt;

&lt;h3 id="implementation-in-rails"&gt;Implementation in rails&lt;/h3&gt;

&lt;p&gt;Now that we have found an index definition that helped us improving our query, let’s add it to our production database. It’s pretty easy, rails migration DSL can already handle partial indexes :&lt;/p&gt;

&lt;pre&gt;&lt;code class="prettyprint lang-rb"&gt;    add_index :messages, [ :id, :created_at ],
              where: &amp;quot;is_abusive = TRUE or is_spam = TRUE&amp;quot;,
              name: &amp;quot;messages_needing_review_idx&amp;quot;,
              algorithm: :concurrently
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The all magic happens in the &lt;code class="prettyprint"&gt;where&lt;/code&gt; clause. The index will be written to only when the filter condition is true.&lt;/p&gt;

&lt;h2 id="measure-consolidate"&gt;Measure &amp;amp; consolidate&lt;/h2&gt;

&lt;p&gt;Running the query in production confirmed our issue was fixed. Using the measure we implemented in the first section was a good confirmation your partial index helped.&lt;/p&gt;

&lt;p&gt;Partial indexes are fragile: if the filter condition changes, the query planner will not be able to utilize them effectively. 
Consequently, this could lead to the same issue resurfacing in the future. 
I want to ensure that future developers, using this query will consistently hit the index. 
I added a spec checking if the query planner keeps using the partial index in the future. Adding a postgres view may be another step torward the coupling of the index with the query.&lt;/p&gt;

&lt;pre&gt;&lt;code class="prettyprint lang-ruby"&gt;  it &amp;quot;uses the partial index&amp;quot; do
    expect(query.explain).to include(&amp;#39;Index Scan Backward using messages_needing_review_idx on messages&amp;#39;)
  end
&lt;/code&gt;&lt;/pre&gt;

&lt;hr/&gt;

&lt;h3 id="conclusion"&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;In summary, creating a partial index on my &lt;code class="prettyprint"&gt;messages&lt;/code&gt; table significantly improved query performance. &lt;/p&gt;

&lt;p&gt;Keep in mind that adding an index  will add write overhead (otherwise we would add indexes for every single query).&lt;/p&gt;
</content>
  </entry>
</feed>
