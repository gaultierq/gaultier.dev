<!doctype html><html class="h-full"><head><meta charset="utf-8" /><meta content="ie=edge" http-equiv="X-UA-Compatible" /><meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport" /><title>gaultier.dev</title><link href="../../../images/favicon.ico" rel="icon" type="image/x-icon" /><link rel="alternate" type="application/atom+xml" title="Atom Feed" href="/feed.xml" /><script data-website-id="58e0c11c-be1a-4563-a97f-9005d206fd48" defer="" src="https://umami.canonic.fr/script.js"></script><link href="../../../stylesheets/site.css" rel="stylesheet" /><script type="importmap">
  {
  "imports": {
    "@hotwired/stimulus": "https://unpkg.com/@hotwired/stimulus/dist/stimulus.js",
    "tocbot": "https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.30.0/tocbot.min.js",
    "highlight.js": "https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/es/highlight.min.js"
  }
}


</script>
<script
  type="module"
  src="../../../javascripts/site.js"
></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/default.min.css" rel="stylesheet" /><link href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.30.0/tocbot.css" rel="stylesheet" /></head><body class="h-full flex flex-col"><header><nav class="p-12 bg-gray-100"><a href="../../../"><span class="font-serif text-2xl font-semibold whitespace-nowrap text-gray-800">gaultier.dev</span></a><div><ul class="flex gap-4 mt-8 text-gray-800"><li><a href="/">Home</a></li><li><a href="/about">About</a></li><li><a href="/links">Links</a></li></ul></div></nav></header><main class="flex-1 p-12"><div data-controller="lol"><article class="prose prose-code:before:hidden prose-code:after:hidden prose-headings:font-serif js-toc-content" data-controller="tocbot highlightjs" data-lol-target="source"><h1 id="how-to-shield-your-app-from-a-rogue-sidekiq-job">How to Shield Your App from a Rogue Sidekiq Job</h1>

<p>Sidekiq is great for background processing—but a single misbehaving job (infinite loops, massive data scans, N+1 storms…) can choke your entire pipeline. Restarting Sidekiq only brings the offender right back. </p>

<p><strong>How do you quarantine a bad job so that it can’t block all your critical work?</strong></p>

<h2 id="a-real-world-incident">A Real-World Incident</h2>

<p>A user submitted a report with a huge date range—no sanitization—and our <code class="prettyprint">ReportJob</code> tried to load terabytes of data. CPU spiked, threads stalled, and everything ground to a halt. Killing and restarting Sidekiq pods simply requeued the same job, and the outage persisted. We needed a way to isolate that single problematic worker.</p>

<h2 id="introducing-a-quarantine-queue">Introducing a “Quarantine” Queue</h2>

<p>The idea is simple: route suspect jobs into a low-concurrency queue serviced by a dedicated Sidekiq process. They’re still retried, but can’t block your high-priority queues.</p>

<h3 id="1-moving-jobs-to-the-quarantine-queue-manually">1. Moving jobs to the quarantine queue manually</h3>

<p>We begin by intercepting job enqueuing and swapping their queue name if they match an ENV-driven “quarantine” list:</p>
<div class="not-prose">
  <pre><code class="language-ruby">class QuarantineMiddleware
  def call(worker, job, queue, redis_pool)
    if ENV.fetch(&#39;QUARANTINED_JOBS&#39;, &#39;&#39;).split(&#39;;&#39;).include?(job[&#39;class&#39;])
      job[&#39;queue&#39;] = &#39;quarantine&#39;
    end
    yield
  end
end

Sidekiq.configure_client do |config|
  config.client_middleware { |chain| chain.add QuarantineMiddleware }
end
</code></pre>
</div>

<p>Set
<code class="prettyprint">
QUARANTINED_JOBS=ReportJob;MegaLoopJob
</code>
to start shunting those jobs into quarantine.</p>

<p><a href="https://www.youtube.com/watch?v=bvdWPGQ8cEA&amp;t=229s">Inspired by this deep-dive on isolating memory-leak jobs.</a>.</p>

<h3 id="2-handling-infinite-loops-on-restart">2. Handling Infinite Loops on Restart</h3>

<p>That client middleware only works on new enqueues; it can’t catch jobs Sidekiq auto-requeues on shutdown. To trap long-running jobs after a pod restart, we use a server middleware:</p>
<div class="not-prose">
  <pre><code class="language-ruby">class LongRunningJobInterceptor
  QUARANTINE_QUEUE = &#39;quarantine&#39;

  def call(worker, job, queue)
    jid = job[&#39;jid&#39;]
    if queue != QUARANTINE_QUEUE &amp;&amp; should_quarantine?(jid)
      # Requeue safely into quarantine
      worker.class.set(queue: QUARANTINE_QUEUE).perform_async(*job[&#39;args&#39;])
      return
    end

    track_start(jid)
    begin
      yield
    rescue Sidekiq::Shutdown
      raise
    ensure
      untrack(jid)
    end
  end

  private

  def should_quarantine?(jid)
    start_time = Sidekiq.redis { |r| r.get(&quot;job:#{jid}:started_at&quot;).to_time }
    (Time.current - start_time) &gt; 1.hour
  end

  def track_start(jid)
    Sidekiq.redis { |r| r.set(&quot;job:#{jid}:started_at&quot;, Time.current) }
  end

  def untrack(jid)
    Sidekiq.redis { |r| r.del(&quot;job:#{jid}:started_at&quot;) }
  end
end

Sidekiq.configure_server do |config|
  config.server_middleware { |chain| chain.add LongRunningJobInterceptor }
end
</code></pre>
</div>

<p>Now, whenever you restart Sidekiq, any job that had been running “too long” automatically moves into your quarantine queue instead of clogging default or critical queues.</p>

<h3 id="3-automating-detection-restart">3. Automating Detection &amp; Restart</h3>

<p>Maintaining a list of known bad actors is helpful, but it still leaves you manually updating ENV variables and restarting processes when things go awry. What if we could automate both detection and remediation of runaway jobs?
Let’s watch for runaway jobs and trigger a graceful process restart.</p>

<h4 id="3-a-why-not-simple-timeouts">3.a Why Not Simple Timeouts?</h4>

<p>Some teams rely on hard timeouts (<code class="prettyprint">Sidekiq::JobTimeout</code>) or even OS-level kills. Unfortunately, these can:</p>

<ul>
<li>Abort mid-transaction, leaving partial writes or queued messages.</li>
<li>Leak resources (DB connections, file handles), causing pool exhaustion.</li>
</ul>

<h4 id="3-b-soft-timeout-monitoring">3.b Soft Timeout Monitoring</h4>

<p>Instead of killing the job mid-flight, let’s signal our orchestrator to restart the entire Sidekiq process, then quarantine repeat offenders on the next run.</p>

<ol>
<li>Track job start times in a shared registry.</li>
<li>Periodically scan for jobs exceeding a configurable threshold (e.g. 2 minutes).</li>
<li>Flag the Sidekiq process as unhealthy (e.g. by touching a file).</li>
<li>Let Kubernetes or Systemd detect the unhealthy marker and recycle the pod or service.</li>
</ol>
<div class="not-prose">
  <pre><code class="language-ruby">class SidekiqMonitor
  HEALTH_FILE = Rails.root.join(&#39;tmp/sidekiq_unhealthy&#39;).freeze

  def self.start
    Thread.new(name: &#39;sidekiq-monitor&#39;) do
      loop do
        RunningJobs.list.each do |job|
          if Time.current - job[:started_at] &gt; 2.minutes
            FileUtils.touch(HEALTH_FILE)
            break
          end
        end
        sleep 1
      end
    end
  end
end

class RunningJobs
  @jobs = Concurrent::Map.new

  def call(worker, job, queue)
    @jobs[Thread.current.object_id] = { jid: job[&#39;jid&#39;], started_at: Time.current }
    yield
  ensure
    @jobs.delete(Thread.current.object_id)
  end

  def self.list
    @jobs.values
  end
end

# In config/initializers/sidekiq.rb
Sidekiq.configure_server do |config|
  config.server_middleware { |chain| chain.add RunningJobs }
  SidekiqMonitor.start
end
</code></pre>
</div>

<ul>
<li><p>Kubernetes (or Systemd) watches <code class="prettyprint">tmp/sidekiq_unhealthy</code> and gracefully restarts the process.</p></li>
<li><p>On restart, our <code class="prettyprint">LongRunningJobInterceptor</code> (from step 2) reroutes any job that previously ran too long into the quarantine queue.</p></li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>By combining:</p>

<ol>
<li>Client-side queue rerouting (for future enqueues),</li>
<li>Server-side interception (for jobs requeued on shutdown), and</li>
<li>Automated health monitoring with liveness probes,</li>
</ol>

<p>you can ensure that one rogue Sidekiq job never brings down your entire background pipeline.</p>
</article><aside class="hidden sm:block fixed w-1/3 fixed right-0 top-0 bottom-0 bg-gray-800"><div class="h-1/3 bg-gray-800"><div class="js-toc text-white p-8"></div><div class="relative"><div class="peer prose dark:prose-invert" data-lol-target="expansion"></div><div class="absolute top-0 right-0 peer-[:empty]:hidden cursor-pointer" data-action="click->lol#clear"><svg class="size-6" fill="none" stroke="white" stroke-width="1.5" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M6 18 18 6M6 6l12 12" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></div></aside></div></main><footer><nav class="bg-gray-100 p-12"><div><ul class="flex gap-4 mt-8 text-gray-800"><li><a href="../../../feed.xml">rss</a></li></ul></div></nav></footer></body></html>