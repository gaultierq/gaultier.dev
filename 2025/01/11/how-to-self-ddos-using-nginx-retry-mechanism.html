<!doctype html><html class="h-full"><head><meta charset="utf-8" /><meta content="ie=edge" http-equiv="X-UA-Compatible" /><meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport" /><title>gaultier.dev</title><link rel="alternate" type="application/atom+xml" title="Atom Feed" href="/feed.xml" /><script data-website-id="58e0c11c-be1a-4563-a97f-9005d206fd48" defer="" src="https://umami.canonic.fr/script.js"></script><link href="../../../stylesheets/site.css" rel="stylesheet" /><script type="importmap">
  {
  "imports": {
    "@hotwired/stimulus": "https://unpkg.com/@hotwired/stimulus/dist/stimulus.js",
    "tocbot": "https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.30.0/tocbot.min.js",
    "highlight.js": "https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/es/highlight.min.js"
  }
}


</script>
<script
  type="module"
  src="../../../javascripts/site.js"
></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/default.min.css" rel="stylesheet" /><link href="https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.30.0/tocbot.css" rel="stylesheet" /></head><body class="h-full flex flex-col"><header><nav class="p-12 bg-gray-100"><a href="../../../"><span class="font-serif text-2xl font-semibold whitespace-nowrap text-gray-800">gaultier.dev</span></a><div><ul class="flex gap-4 mt-8 text-gray-800"><li><a href="/">Home</a></li><li><a href="/about">About</a></li></ul></div></nav></header><main class="flex-1 p-12"><div data-controller="lol"><article class="prose prose-code:before:hidden prose-code:after:hidden prose-headings:font-serif js-toc-content" data-controller="tocbot highlightjs" data-lol-target="source"><h1 id="self-ddos-ing-with-long-running-requests-and-nginx">Self DDOS-ing with long-running requests and Nginx</h1>

<p>What if I told you that a single Nginx setting turned a long running request into a self-DoS?
That&#39;s exactly what happened on our cluster… Here&#39;s a breakdown of what happened.</p>

<h2 id="the-incident">The Incident</h2>

<p>Last week, our monitoring alerted us to a sudden CPU spike on the database node. The culprit: long-running SQL queries triggered when customers generated PDF reports for their dashboards.</p>

<p>Instead of enqueueing these tasks as background jobs, our Rails app processed them synchronously within the request lifecycle. Since report generation can take up to 40 minutes, frustrated users refreshed their browsers after a few seconds—triggering new requests and compounding the load.</p>

<p>Over time, these repeated attempts exhausted both web and database resources, degrading performance cluster-wide.</p>

<h2 id="investigation">Investigation</h2>

<p>I reproduced the issue by generating a report myself. 
After about three minutes, my browser returned a 504 Gateway Timeout. 
Curious about where that timeout lived, I inspected our Nginx Ingress Controller settings and found: </p>
<div class="not-prose">
  <pre><code class="language-nginx">proxy_read_timeout 60s;
proxy_next_upstream_tries 3;
</code></pre>
</div>

<p>Here’s what happened:</p>

<ol>
<li>A request hits Nginx and is proxied to our Rails app.</li>
<li>If no response arrives within 60 seconds, Nginx retries—up to three times.</li>
<li>Meanwhile, the Rails process keeps generating the report for its full duration.</li>
<li>Frustrated users hit refresh, spawning new requests on top of Nginx’s retries.</li>
</ol>

<p>The result? Each report could be processed 3 times per user refresh, quickly overwhelming the cluster.</p>

<h2 id="the-fix">The Fix</h2>

<p>While moving report generation to a background job remains the long-term solution, we needed a quick mitigation to prevent another CPU spike.
We decided to deactivate Nginx’s retry logic temporarily. By reducing tries from three to none, we could immediately cut the redundant load by two-thirds.</p>
<div class="not-prose">
  <pre><code class="language-yaml"># Ingress annotations
nginx.ingress.kubernetes.io/proxy-next-upstream-tries: &quot;1&quot;
</code></pre>
</div>

<h2 id="bonus-a-cautionary-ai-tale">Bonus: a Cautionary AI Tale</h2>

<p>Eager for a quick fix, I turned to ChatGPT for advice. 
It confidently recommended <code class="prettyprint">proxy_next_upstream_tries: 0</code>, which I assumed would disable retries completely.
However, the Nginx docs revealed that setting this value to 0 actually causes Nginx to retry the request infinitely every 60 seconds. The change sent our CPU usage through the roof until I reverted the Helm chart.</p>

<h2 id="next-steps">Next Steps</h2>

<ul>
<li>Asynchronous Processing: Refactor report generation into background jobs with progress notifications.</li>
<li>Timeouts and Circuit Breakers: Implement application-level timeouts to fail fast.</li>
<li>User Feedback: Show a progress indicator and prevent manual refreshes during long-running tasks.</li>
</ul>

<p>This incident reinforced how a small misconfiguration in Nginx can amplify inherent flaws in application design. A careful read of documentation and a temporary tweak saved us from another spike—while we work on the long-term fix.</p>
</article><aside class="hidden sm:block fixed w-1/3 fixed right-0 top-0 bottom-0 bg-gray-800"><div class="h-1/3 bg-gray-800"><div class="js-toc text-white p-8"></div><div class="relative"><div class="peer prose dark:prose-invert" data-lol-target="expansion"></div><div class="absolute top-0 right-0 peer-[:empty]:hidden cursor-pointer" data-action="click->lol#clear"><svg class="size-6" fill="none" stroke="white" stroke-width="1.5" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M6 18 18 6M6 6l12 12" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></div></aside></div></main><footer><nav class="bg-gray-100 p-12"><div><ul class="flex gap-4 mt-8 text-gray-800"><li><a href="../../../feed.xml">rss</a></li></ul></div></nav></footer></body></html>